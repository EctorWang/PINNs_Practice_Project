{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a6bb87",
   "metadata": {},
   "source": [
    "## 悬臂梁一端固支的情况，另一端自由端"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad55904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "import xlrd\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef97dbb",
   "metadata": {},
   "source": [
    "## 定义神经网络\n",
    "\n",
    "使用循环的方式定义的全连接神经网络\n",
    "采用前向传播的方式计算出输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61291247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Neural Network Model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = torch.tanh(self.layers[i](x))\n",
    "        x = self.layers[-1](x)  # Last layer without activation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca1ecf",
   "metadata": {},
   "source": [
    "## 定义整体的神经网络结构和输入的参数\n",
    "还有各个网络的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINN Model Class\n",
    "class PINN_model:\n",
    "    def __init__(self, layers, X, q, X_uc, u_c, X_ac, a_c, X_kc, k_c):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        self.q = torch.tensor(q, dtype=torch.float32).to(self.device)\n",
    "        self.X_uc = torch.tensor(X_uc, dtype=torch.float32).to(self.device)\n",
    "        self.u_c = torch.tensor(u_c, dtype=torch.float32).to(self.device)\n",
    "        self.X_ac = torch.tensor(X_ac, dtype=torch.float32).to(self.device)\n",
    "        self.a_c = torch.tensor(a_c, dtype=torch.float32).to(self.device)\n",
    "        self.X_kc = torch.tensor(X_kc, dtype=torch.float32).to(self.device)\n",
    "        self.k_c = torch.tensor(k_c, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Initialize networks\n",
    "        self.net_u = NeuralNet(layers[0]).to(self.device)\n",
    "        self.net_a = NeuralNet(layers[1]).to(self.device)\n",
    "        self.net_k = NeuralNet(layers[2]).to(self.device)\n",
    "        self.net_Q = NeuralNet(layers[3]).to(self.device)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.get_params(), lr=1e-3)\n",
    "        self.EI = 1.0  # Material property\n",
    "\n",
    "        # Loss Logs\n",
    "        self.loss_log = []\n",
    "        self.loss_c_log = []\n",
    "        self.loss_f_log = []\n",
    "\n",
    "    # 将Network其中的参数传递给优化器，这里代表的是各个模型中的参数\n",
    "    def get_params(self):\n",
    "        return list(self.net_u.parameters()) + list(self.net_a.parameters()) + \\\n",
    "               list(self.net_k.parameters()) + list(self.net_Q.parameters())\n",
    "\n",
    "    # 定义了求道的参数x\n",
    "    def net_f(self, x):\n",
    "        x.requires_grad = True\n",
    "        Q = self.net_Q(x)  # 剪力\n",
    "        k = self.net_k(x)  # 曲率，是为了后面计算弯矩\n",
    "        a = self.net_a(x)  # 角度\n",
    "        u = self.net_u(x)  # 位移\n",
    "        \n",
    "        # 计算弯矩\n",
    "        M = self.EI * k\n",
    "\n",
    "        # Gradients，求导，各个参数之间的导数关系，这里就可以体现出来\n",
    "        Q_x = torch.autograd.grad(Q, x, grad_outputs=torch.ones_like(Q), create_graph=True, retain_graph=True)[0]\n",
    "        M_x = torch.autograd.grad(M, x, grad_outputs=torch.ones_like(M), create_graph=True, retain_graph=True)[0]\n",
    "        a_x = torch.autograd.grad(a, x, grad_outputs=torch.ones_like(a), create_graph=True, retain_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # 导数之间的关系，也就是控制方程之间的联系a  u k Q 的关系，最后根据关系得到损失函数的值为多少\n",
    "        f_Q_q = Q_x + self.q\n",
    "        f_M_Q = M_x - Q\n",
    "        f_a_k = a_x + k\n",
    "        f_u_k = u_x - a\n",
    "\n",
    "        # 返回的是损失函数的值，也就是实际的方程的值\n",
    "        return f_Q_q, f_M_Q, f_a_k, f_u_k\n",
    "\n",
    "    def train(self, nIter=10000):\n",
    "        # Training loop 对于模型构建好，后面开始进行训练了\n",
    "        for it in range(nIter):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Compute losses 将训练的模型的数据进行预测，得到预测值\n",
    "            u_c_pred = self.net_u(self.X_uc)\n",
    "            a_c_pred = self.net_a(self.X_ac)\n",
    "            k_c_pred = self.net_k(self.X_kc)\n",
    "\n",
    "            # 得到各个导数的值进行分析\n",
    "            f_Q_q, f_M_Q, f_a_k, f_u_k = self.net_f(self.X)\n",
    "\n",
    "            # 计算出来的导数，根据物理方程的关系，计算他们之间损失函数的值\n",
    "            # 一个是物理方程的损失函数，一个是边界条件的损失函数，还有数据的损失函数\n",
    "            \n",
    "            ##----------------------------------这里的损失函数的定义是相当重要的，模型的好坏和训练是否容易收敛跟这里是息息相关的-------------------###\n",
    "            loss_c = torch.mean((u_c_pred - self.u_c) ** 2) + torch.mean((k_c_pred - self.k_c) ** 2)\n",
    "            loss_f = torch.mean(f_Q_q ** 2) + torch.mean(f_M_Q ** 2) + torch.mean(f_a_k ** 2) + torch.mean(f_u_k ** 2)\n",
    "            loss = loss_c + loss_f\n",
    "            ##---------------------------------------------------------------------------------------###\n",
    "\n",
    "            # 误差反向传播，将误差值对梯度进行下降处理\n",
    "            loss.backward()\n",
    "            # 更新 Network 的参数\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Logging，记录反向传播的误差值，记录训练的次数，以及损失函数的值\n",
    "            self.loss_log.append(loss.item())\n",
    "            self.loss_c_log.append(loss_c.item())\n",
    "            self.loss_f_log.append(loss_f.item())\n",
    "\n",
    "            # 根据训练到达的次数，打印出训练的次数，以及损失函数的值\n",
    "            if it % 100 == 0:\n",
    "                print(f\"Iter {it}, Loss_c: {loss_c:.3e}, Loss_f: {loss_f:.3e}, Total Loss: {loss:.3e}\")\n",
    "\n",
    "    # 根据训练的模型进行相关值的预测，返回预测值\n",
    "    def predict(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            u = self.net_u(x).cpu().numpy()\n",
    "            a = self.net_a(x).cpu().numpy()\n",
    "            k = self.net_k(x).cpu().numpy()\n",
    "            Q = self.net_Q(x).cpu().numpy()\n",
    "        return Q, k, a, u\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8949b",
   "metadata": {},
   "source": [
    "## 上面定义好整个模型和相关的模型的细节\n",
    "- 接下来要开始数据的处理和训练了\n",
    "1. 数据处理\n",
    "- 训练数据集和测试数据集的划分\n",
    "- 数据集的预处理\n",
    "- 数据集的加载\n",
    "- 数据集的迭代\n",
    "- 数据集的可视化\n",
    "- 数据集的保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ecb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    ## ------------------------ 各个 Network 的隐藏层相关的信息 ---------------------------------##\n",
    "    layers = [[1] + 3 * [10] + [1], [1] + 3 * [10] + [1], [1] + 3 * [10] + [1], [1] + 3 * [10] + [1]]\n",
    "\n",
    "    # 相关参数的处理和分析\n",
    "    # Generate data using the formula w = -M_epsilon * x^2 / (2 * EI)\n",
    "    M_epsilon = 1.0  # Example value for M_epsilon\n",
    "    EI = 1.0  # Example value for EI\n",
    "\n",
    "    # ------------- 准备原始数据：训练的数据，验证的数据 ------------- #\n",
    "    X_star = np.linspace(0, 1, 1001).reshape(-1, 1)\n",
    "    u_fem = -M_epsilon * X_star**2 / (2 * EI)  # Generate displacement data\n",
    "    a_fem = -M_epsilon * X_star / (EI)  # Generate corner data\n",
    "    k_fem = -M_epsilon * np.ones_like(X_star) / (EI)  # Generate curvature data\n",
    "\n",
    "    # ------------- 这里的数据在整体的样本点库中随机的选取 50个 ------------- #\n",
    "    N_f = 50\n",
    "    idx = np.random.choice(X_star.shape[0], N_f, replace=False)\n",
    "    X_train = X_star[idx, :]\n",
    "\n",
    "    # ------------- 绘制整个杆上选取的数据点的可视化的图形 ------------- #\n",
    "    # Plot training points\n",
    "    plt.figure(figsize=(10, 1), dpi=300)\n",
    "    plt.plot([0, 1], [0, 0], '^', markersize=10, label='Boundary')\n",
    "    plt.xlim(-0.05, 1.05)\n",
    "    plt.ylim(-0.7, 2.0)\n",
    "    plt.plot(X_train, np.zeros(X_train.shape), 'o', markersize=8, alpha=0.5, label='Random data')\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('Location x')\n",
    "    plt.legend(fontsize=12, ncol=2)\n",
    "    plt.savefig('beam1_model1_training_points_high_res.png', dpi=300, bbox_inches='tight')  # Save the plot\n",
    "    plt.show()\n",
    "\n",
    "    # ------------- 根据得到的理论解，确定各个边界条件的数值 ------------- #\n",
    "    q = np.full_like(X_train, 0)\n",
    "    X_uc = np.array([[0.0], [1.0]])\n",
    "    u_c = np.array([[u_fem[0, 0]], [u_fem[-1, 0]]])\n",
    "    X_ac = np.array([[0.0], [1.0]])\n",
    "    a_c = np.array([[a_fem[0, 0]], [a_fem[-1, 0]]])\n",
    "    X_kc = np.array([[0.0], [1.0]])\n",
    "    k_c = np.array([[-k_fem[0, 0]], [-k_fem[-1, 0]]])\n",
    "\n",
    "    # ------------- 模型数据的输入和相关参数的初始化分析 ------------- #\n",
    "    model = PINN_model(layers, X_train, q, X_uc, u_c, X_ac, a_c, X_kc, k_c)\n",
    "    # 模型开始训练，且训练的次数为8000次\n",
    "    model.train(8000)\n",
    "\n",
    "    # 模型训练完成后，进行相关的预测\n",
    "    # 这里预测的点和训练的点完全不一样，训练只是采取其中少数的点，预测就是全部的点进行预测\n",
    "    Q_pred, k_pred, a_pred, u_pred = model.predict(X_star)\n",
    "\n",
    "    # ------------- 绘制预测的结果和真实的结果进行对比 ------------- #\n",
    "    # 这里的数据进行分析的可视化\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    plt.plot(X_star, u_fem, label='Generated Data', linewidth=2)\n",
    "    plt.plot(X_star, u_pred, '--', label='PINN', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.savefig('beam1_model1_generated_data_vs_PINN_high_res.png', dpi=300, bbox_inches='tight')  # Save the plot\n",
    "    plt.show()\n",
    "\n",
    "    # ------------- 除了绘制位移的数据，还可以绘制力矩、转角的图形对比 ------------- #\n",
    "    \n",
    "\n",
    "    # ------------- 绘制训练过程中损失值的可视化 ------------- #\n",
    "    # Plot the loss values during training\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(model.loss_log, label='Total Loss', linewidth=2, color='blue')\n",
    "    plt.plot(model.loss_c_log, label='Constraint Loss', linewidth=2, color='lightsalmon', alpha=0.5)\n",
    "    plt.plot(model.loss_f_log, label='Physics Loss', linewidth=2, color='lightgray', alpha=0.5)\n",
    "    plt.yscale('log')  # Log scale for better visualization of loss reduction\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('beam1_model1_training_losses_high_res.png', dpi=300, bbox_inches='tight')  # Save the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aff3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
